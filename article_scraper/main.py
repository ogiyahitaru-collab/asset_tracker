import os, json, argparse, time, re
from datetime import datetime, timezone, timedelta
import requests, feedparser

from utils.scraper import scrape_single_article
from utils.notion.post_to_notion import post_to_notion

# summarizer ã¯ä»»æ„ï¼ˆã‚ã‚Œã°ä½¿ã†ï¼‰
try:
    from utils.summarizer_stub import summarize as ai_summarize
except Exception:
    ai_summarize = None

SENT_LOG_PATH = "output/sent_log.json"

# ====== ã‚¿ã‚°æ¨å®š ======
INDUSTRY_RULES = [
    (r"ç¥æˆ¸è£½é‹¼|ç¥æˆ¸é‹¼|Kobe Steel|Kobelco", "é‰„é‹¼"),
    (r"JFE|æ—¥æœ¬è£½é‰„|Nippon Steel|è£½é‰„|ç²—é‹¼", "é‰„é‹¼"),
    (r"çŸ³æ²¹|åŸæ²¹|ã‚¬ã‚¹|ã‚¨ãƒãƒ«ã‚®ãƒ¼|LNG|å†ç”Ÿå¯èƒ½|å†ã‚¨ãƒ|å¤ªé™½å…‰|é¢¨åŠ›", "ã‚¨ãƒãƒ«ã‚®ãƒ¼"),
    (r"åŠå°ä½“|ãƒãƒƒãƒ—|ãƒ•ã‚¡ã‚¦ãƒ³ãƒ‰ãƒª|å‰å·¥ç¨‹|å¾Œå·¥ç¨‹", "åŠå°ä½“"),
    (r"åŒ–å­¦|æ¨¹è„‚|å¡©ãƒ“|ãƒãƒª|ã‚±ãƒŸã‚«ãƒ«", "åŒ–å­¦"),
    (r"ç‰©æµ|æµ·é‹|ã‚³ãƒ³ãƒ†ãƒŠ|é‹è³ƒ", "è¼¸é€"),
]

TYPE_RULES = [
    (r"æ¸›ç”£|ç”Ÿç”£èª¿æ•´|ç¨¼åƒç‡|åœæ­¢è§£é™¤", "ç”Ÿç”£"),
    (r"æ¸›ç”£|ç”Ÿç”£èª¿æ•´|ç¨¼åƒç‡|åœæ­¢è§£é™¤", "ç”Ÿç”£"),
    (r"æ±ºç®—|æ¥­ç¸¾|åˆ©ç›Š|å–¶æ¥­ç›Š|ç´”åˆ©ç›Š|é€šæœŸè¦‹é€šã—|ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹|ä¸‹æ–¹ä¿®æ­£|ä¸Šæ–¹ä¿®æ­£", "æ±ºç®—"),
    (r"æŠ•è³‡|æ–°å·¥å ´|è¨­å‚™æŠ•è³‡|å¢—ç”£|ãƒ©ã‚¤ãƒ³æ–°è¨­|è³‡æœ¬æ”¯å‡º|capex", "è¨­å‚™æŠ•è³‡"),
    (r"è²·å|å£²å´|M&A|è³‡æœ¬ææº|å­ä¼šç¤¾åŒ–|åˆä½µ", "M&A"),
    (r"äº‹æ•…|ç«ç½|çˆ†ç™º|ä¸ç¥¥äº‹|ãƒ‡ãƒ¼ã‚¿æ”¹ã–ã‚“|å“è³ªå•é¡Œ|ãƒªã‚³ãƒ¼ãƒ«", "äº‹æ•…ãƒ»ä¸ç¥¥äº‹"),
    (r"è¦åˆ¶|é–¢ç¨|é–¢ç¨ãƒªã‚¹ã‚¯|åˆ¶è£|è¼¸å‡ºç®¡ç†|åãƒ€ãƒ³ãƒ”ãƒ³ã‚°|ç’°å¢ƒè¦åˆ¶|ç‚­ç´ ç¨|CBAM", "è¦åˆ¶ãƒ»æ”¿ç­–"),
    (r"æ–°è£½å“|æ–°ç´ æ|ç™ºè¡¨|æŠ«éœ²|ä¾›çµ¦é–‹å§‹|å—æ³¨", "æ–°è£½å“/å—æ³¨"),
]

IMPACT_RULES = [
    (r"ä¸‹æ–¹ä¿®æ­£|åœæ­¢|åœæ­¢ã¸|æ“æ¥­åœæ­¢|çˆ†ç™º|å¤§å¹…ä¸‹è½|å¤§å¹…å¢—ç¨|åˆ¶è£å¼·åŒ–|é–¢ç¨(å¼•ãä¸Šã’|å¼·åŒ–)", "é«˜"),
    (r"ä¸Šæ–¹ä¿®æ­£|å¢—ç”£|æ–°å·¥å ´|å¤§å‹æŠ•è³‡|å¤§å‹å—æ³¨|åˆå¼è¨­ç«‹|å¤§å‹M&A", "é«˜"),
    (r"èª¿æŸ»é–‹å§‹|æ¤œè¨|æ–¹é‡|ä¸€éƒ¨å ±é“|è¦³æ¸¬", "ä¸­"),
]

def infer_category(rules, text):
    for pat, label in rules:
        if re.search(pat, text, re.IGNORECASE):
            return label
    return None

def infer_tags(title, body, keyword=""):
    base = " ".join([title or "", keyword or "", (body or "")[:2000]])
    industry = infer_category(INDUSTRY_RULES, base) or "æœªåˆ†é¡"
    typ      = infer_category(TYPE_RULES,      base) or "ãã®ä»–"
    impact   = infer_category(IMPACT_RULES,    base) or "ä¸­"
    # ã€Œé¸æŠã€ã«å…¥ã‚Œã‚‹åˆæˆã‚¿ã‚°ï¼ˆSelectã¯1å€¤ã®ã¿ï¼‰
    select_value = f"æ¥­ç•Œ:{industry}ï½œã‚¿ã‚¤ãƒ—:{typ}ï½œå½±éŸ¿åº¦:{impact}"
    return select_value, {"industry": industry, "type": typ, "impact": impact}

# ====== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
def load_sent_log():
    if os.path.exists(SENT_LOG_PATH):
        try:
            with open(SENT_LOG_PATH, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_sent_log(urls:set):
    os.makedirs(os.path.dirname(SENT_LOG_PATH), exist_ok=True)
    with open(SENT_LOG_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(list(urls)), f, ensure_ascii=False, indent=2)

def jst_today():
    return datetime.now(timezone(timedelta(hours=9))).date().isoformat()

def to_ymd(iso_like:str|None) -> str:
    if not iso_like:
        return jst_today()
    try:
        iso_like = iso_like.replace("Z","+00:00")
        dt = datetime.fromisoformat(iso_like)
        return dt.astimezone(timezone(timedelta(hours=9))).date().isoformat()
    except Exception:
        return jst_today()

def summarize_text(title:str, body:str) -> str:
    body = (body or "").strip()
    if ai_summarize and len(body) >= 40:
        try:
            return ai_summarize(body)
        except Exception:
            pass
    lead = (body.split("\n")[0] if body else title) or "è¦ç´„ãªã—"
    return (lead[:200] + ("â€¦" if len(lead)>200 else ""))

def resolve_bing_apiclick(url:str) -> str:
    try:
        resp = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=12, allow_redirects=True)
        return resp.url
    except Exception:
        return url

def iter_bing_news_urls(keyword:str, limit:int):
    q = requests.utils.quote(keyword)
    rss = f"https://www.bing.com/news/search?q={q}+site:reuters.com&format=rss"
    feed = feedparser.parse(rss)
    seen = set()
    for e in feed.entries:
        if len(seen) >= limit: break
        u = e.link
        if "sitemap" in u.lower(): continue
        u = resolve_bing_apiclick(u)
        if "reuters.com" not in u: continue
        if u in seen: continue
        seen.add(u)
        yield u

# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† ======
def process_url(url:str, sent:set, keyword:str=""):
    if url in sent:
        print(f"â­ï¸ æ—¢é€ä¿¡ã‚¹ã‚­ãƒƒãƒ—: {url}")
        return False

    print(f"ğŸ“„ Processing: {url}")
    art = scrape_single_article(url)  # {url/final_url, title, text, published_at?}
    final_url = art.get("final_url") or art.get("url") or url
    title = art.get("title") or "No Title"
    text  = art.get("text")  or ""
    pub   = to_ymd(art.get("published_at"))

    # ã‚µãƒãƒªï¼†ã‚¿ã‚°
    summary = summarize_text(title, text)
    select_value, tag_detail = infer_tags(title, text, keyword)

    # Notionç™»éŒ²ï¼ˆDB: title / URL / summary_ja / æ—¥ä»˜ / é¸æŠï¼‰
    ok = post_to_notion(
        title=title,
        url=final_url,
        summary=f"{summary}\n\n[è‡ªå‹•ã‚¿ã‚°] æ¥­ç•Œ:{tag_detail['industry']}ï½œã‚¿ã‚¤ãƒ—:{tag_detail['type']}ï½œå½±éŸ¿åº¦:{tag_detail['impact']}",
        date_str=pub,
        tag_name=select_value
    )
    if ok:
        sent.add(final_url)
        save_sent_log(sent)
    return ok

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--url", help="å˜ä¸€URLã‚’å‡¦ç†")
    parser.add_argument("--keyword", help="Bing News RSSã§æ¤œç´¢ã—ã¦å‡¦ç†")
    parser.add_argument("--limit", type=int, default=5, help="å–å¾—ä»¶æ•°ï¼ˆkeywordæ™‚ï¼‰")
    args = parser.parse_args()

    sent = load_sent_log()

    if args.url:
        process_url(args.url, sent)
        return

    if args.keyword:
        count = 0
        for u in iter_bing_news_urls(args.keyword, args.limit):
            process_url(u, sent, keyword=args.keyword)
            count += 1
            time.sleep(1.0)
        if count == 0:
            print("âŒ RSSã‹ã‚‰URLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    parser.print_help()

if __name__ == "__main__":
    main()
