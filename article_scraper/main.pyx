import os
import sys
import json
import argparse
import pandas as pd

# ç›¸å¯¾ãƒ‘ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ç”¨
sys.path.append("./article_scraper")

from scraper import scrape_articles
from summarizer import summarize_articles
from utils.notion import post_to_notion

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
SENT_LOG_PATH = "output/sent_log.json"

def load_sent_log():
    if os.path.exists(SENT_LOG_PATH):
        with open(SENT_LOG_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_sent_log(sent_articles):
    with open(SENT_LOG_PATH, "w", encoding="utf-8") as f:
        json.dump(sent_articles, f, ensure_ascii=False, indent=2)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--limit", type=int, help="é€ä¿¡ã™ã‚‹æœ€å¤§ä»¶æ•°")
    args = parser.parse_args()

    print("ğŸ” è¨˜äº‹ã‚’å–å¾—ä¸­...")
    articles = scrape_articles()

    # é€ä¿¡æ¸ˆã¿URLã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    sent_log = load_sent_log()
    new_articles = [a for a in articles if a.get("url") and a["url"] not in sent_log]

    if args.limit:
        new_articles = new_articles[:args.limit]

    if not new_articles:
        print("â­ï¸ æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return

    print("âœï¸ è¦ç´„ç”Ÿæˆä¸­...")
    summaries = summarize_articles(new_articles)

    os.makedirs("output", exist_ok=True)

    # JSONä¿å­˜
    with open("output/summary.json", "w", encoding="utf-8") as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)

    # CSVä¿å­˜
    df = pd.DataFrame(summaries)
    df.to_csv("output/summary.csv", encoding="utf-8", index=False)

    print("ğŸ“¤ Notionã«é€ä¿¡ä¸­...")
    post_to_notion.main()

    # ãƒ­ã‚°æ›´æ–°
    sent_log += [a["url"] for a in new_articles]
    save_sent_log(sent_log)

    print("âœ… å…¨å‡¦ç†å®Œäº†")

if __name__ == "__main__":
    main()
