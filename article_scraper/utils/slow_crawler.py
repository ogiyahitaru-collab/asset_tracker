import os, time, json, random, pathlib
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# ===== åŸºæœ¬è¨­å®š =====
MIN_DELAY = 20       # æœ€å°å¾…æ©Ÿç§’ï¼ˆå„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ï¼‰
MAX_DELAY = 45       # æœ€å¤§å¾…æ©Ÿç§’ï¼ˆå„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ï¼‰
HARD_BACKOFF_4XX = (120, 300)  # 4xxç³»ã®æ™‚ã«ã¾ã¨ã‚ã¦å¾…ã¤ç§’ï¼ˆmin,maxï¼‰
MAX_RETRIES = 3
TIMEOUT = 30

USER_AGENTS = [
    # PCãƒ–ãƒ©ã‚¦ã‚¶ã£ã½ã„UAã‚’æ•°ç¨®ãƒ©ãƒ³ãƒ€ãƒ ã«
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36",
]

HEADERS_BASE = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
    "Connection": "close",
}

COMMON_SELECTORS = [
    # ã‚ˆãã‚ã‚‹æœ¬æ–‡ã®å…¥ã‚Œç‰©ï¼ˆä¸Šã‹ã‚‰é †ç•ªã«è©¦ã™ï¼‰
    "article",                         # æ±ç”¨
    "[data-testid='Body']",
    "div.article-body", "div.ArticleBody",
    "div.story-content", "div.paywall-article", "div#article-body",
    "div[itemprop='articleBody']",
    "section#content", "main#content",
    ".post-content", ".entry-content", ".c-article__body",
]

# ===== Notionï¼ˆä»»æ„ï¼‰ =====
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
NOTION_DB = os.getenv("NOTION_DATABASE_ID")
use_notion = bool(NOTION_TOKEN and NOTION_DB)
if use_notion:
    try:
        from notion_client import Client as NotionClient
        notion = NotionClient(auth=NOTION_TOKEN)
    except Exception:
        use_notion = False

def safe_sleep(t):
    # ç§’æ•°ã‚’è¦‹ãˆã‚‹ãƒ­ã‚°
    print(f"â³ wait {int(t)}s â€¦")
    time.sleep(t)

def fetch_html(url):
    session = requests.Session()
    headers = HEADERS_BASE.copy()
    headers["User-Agent"] = random.choice(USER_AGENTS)

    last_exc = None
    for attempt in range(1, MAX_RETRIES+1):
        try:
            resp = session.get(url, headers=headers, timeout=TIMEOUT)
            status = resp.status_code

            # æ­£å¸¸
            if 200 <= status < 300 and "text/html" in resp.headers.get("Content-Type", ""):
                return resp.text

            # 429/403: å¼·ã‚ãƒãƒƒã‚¯ã‚ªãƒ•
            if status in (403, 429):
                backoff = random.uniform(*HARD_BACKOFF_4XX)
                print(f"âš  {status} on {url} -> hard backoff {int(backoff)}s")
                safe_sleep(backoff)
            else:
                # 4xx/5xxãã®ä»– â†’ è»½ã„æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                backoff = min(60, 2 ** attempt * 3 + random.uniform(0, 3))
                print(f"âš  status {status} (attempt {attempt}) -> backoff {int(backoff)}s")
                safe_sleep(backoff)

        except requests.RequestException as e:
            last_exc = e
            backoff = min(60, 2 ** attempt * 3 + random.uniform(0, 3))
            print(f"âš  error {e} (attempt {attempt}) -> backoff {int(backoff)}s")
            safe_sleep(backoff)

    raise RuntimeError(f"Failed to fetch: {url} ({last_exc})")

def extract_text(html):
    soup = BeautifulSoup(html, "html.parser")
    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆfallbackã‚ã‚Šï¼‰
    title = (
        (soup.title.string.strip() if soup.title and soup.title.string else None)
        or (soup.find("meta", property="og:title") or {}).get("content")
        or ""
    )

    # æœ¬æ–‡å€™è£œã‚’é †ã«æ¢ç´¢
    body_text = ""
    for css in COMMON_SELECTORS:
        node = soup.select_one(css)
        if node and node.get_text(strip=True):
            # éå‰°ã‚’å‰Šã‚‹
            paragraphs = [p.get_text(" ", strip=True) for p in node.find_all(["p","li"])]
            if not paragraphs:
                paragraphs = [node.get_text(" ", strip=True)]
            body_text = "\n".join(paragraphs).strip()
            if body_text:
                break

    # ä½•ã‚‚å–ã‚Œãªã‘ã‚Œã°ç©ºè¿”ã—
    return title, body_text

def post_to_notion(title, url, summary):
    if not use_notion:
        return
    try:
        notion.pages.create(
            parent={"database_id": NOTION_DB},
            properties={
                "title": {"title": [{"text": {"content": title or "(no title)"}}]},
                "URL": {"url": url},
                "summary_ja": {"rich_text": [{"text": {"content": summary[:1900]}}]},
            },
        )
        print("ğŸŸ¢ Notion ç™»éŒ² OK")
    except Exception as e:
        print(f"ğŸŸ  Notion ç™»éŒ²å¤±æ•—: {e}")

def main():
    urls_path = pathlib.Path("urls.txt")
    urls = [line.strip() for line in urls_path.read_text(encoding="utf-8").splitlines() if line.strip() and not line.strip().startswith("#")]
    out_dir = pathlib.Path("out"); out_dir.mkdir(exist_ok=True)
    out_file = out_dir / "news.jsonl"

    print(f"ğŸ” {len(urls)}æœ¬ã‚’ã‚†ã£ãã‚Šå·¡å›ã—ã¾ã™ï¼ˆ{MIN_DELAY}-{MAX_DELAY}sé–“éš” + ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰")

    for idx, url in enumerate(urls, 1):
        print(f"\nâ€”â€”â€” [{idx}/{len(urls)}] {url}")
        # ãƒ‰ãƒ¡ã‚¤ãƒ³å˜ä½ã§ã®ä¸€èˆ¬çš„ãªç¤¼å„€çš„ã‚¦ã‚§ã‚¤ãƒˆ
        delay = random.uniform(MIN_DELAY, MAX_DELAY)
        safe_sleep(delay)

        try:
            html = fetch_html(url)
            title, body = extract_text(html)

            if not body:
                print("âŒ æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒšãƒ¼ã‚¸ãŒé«˜åº¦ã«å‹•çš„ or åˆ¶é™ä¸­ã®å¯èƒ½æ€§ï¼‰")
                # ãƒ‡ãƒãƒƒã‚°ç”¨ã«HTMLä¿å­˜
                dbg = out_dir / f"page_source_{idx}.html"
                dbg.write_text(html, encoding="utf-8", errors="ignore")
                print(f"ğŸ§© ãƒ‡ãƒãƒƒã‚°HTMLä¿å­˜: {dbg}")
            else:
                preview = body[:500].replace("\n", " ")
                print(f"ğŸŸ¢ æŠ½å‡ºOK: {title[:80]}")
                print(f"   å…ˆé ­500å­—: {preview}")

            record = {
                "url": url,
                "title": title,
                "body": body,
                "fetched_at": int(time.time()),
            }
            with out_file.open("a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")

            # ä»»æ„: Notion
            if body:
                post_to_notion(title, url, body)

        except Exception as e:
            print(f"ğŸ”´ å¤±æ•—: {e}")

    print(f"\nâœ… å®Œäº†: {out_file} ã«è¿½è¨˜ã—ã¾ã—ãŸ")
    print("   ï¼ˆæ¬¡å›ã¯ urls.txt ã‚’æ›´æ–°ã—ã¦åŒã˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚‚ã†ä¸€åº¦å®Ÿè¡Œã™ã‚‹ã ã‘ï¼‰")

if __name__ == "__main__":
    main()
